\section*{Synthesizing Measures of State Preference}

The approach that we introduce here to measuring state preferences starts by assuming that both UN voting and alliance relationships are sources of information on how states relate to one another in the international stage. By accounting for the multiple layers upon which states interact with one another we can synthesize a better measure of state preferences than if we relied on any one measure alone. The idea of using multiple metrics to get a better handle on preferences is not new, in fact, Signorino and Ritter suggested it in introducing S scores, which were designed to allow for aggregation of similarity on multiple dimensions (such as alliances and UN voting). The downside of this extant approach, however, is that it does not account for structural patterns that we often see in relational data. 

Relational data is composed of observations between pairs of actors, or dyads. For both alliance relationships and UN voting, we are able to observe how the actors in the international system  interacted with one another across time. This system of interactions taken in its totality defines a network, and within these types of structures a bevy of research has shown that we need methods that go beyond assuming that interactions are taking place between just two actors in a vacuum \citep{wasserman:faust:1994,snijders:nowicki:1997}. As such we reformulate the problem of determining state preferences in terms of network analysis. The goal of our approach is summarized in Figure~\ref{fig:tensViz}. In the top row, we represent UN voting and alliance patterns at time $t$ in a pair of adjacency matrices that form a multiplex.\footnote{The approach that we describe here can be generalized to a multiplex with more than two dimensions.} This multiplex represents the relations between states across two dimensions, and our goal is to extract a lower dimensional representation that accounts for higher order network patterns such as stochastic equivalence. The end result then will be a single $n \times n$ matrix, where $n$ represents the number of actors, in which the cross-sections denote our predictions for the strength of relations between two countries.

% needs to be modified
\begin{figure}[ht]
	\centering
	% \includegraphics[width=.8\textwidth]{carbonStrat.png}
	\resizebox{.5\textwidth}{!}{\input{dimRedStrat.tex}}
	\caption{Tensor representation of longitudinal dyadic, representational measures. The green and blue colors represent different relational measures and darker shading indicates later time periods. Specifically, we show a tensor with dimensions of $4 \times 4 \times 2 \times 3$, where 4 represents the number of actors, 2 the number of relational measures, and 3 the number of time points.}
	\label{fig:tensViz}
\end{figure}

We generate these predictions through a matrix decomposition technique that estimates a latent Gaussian score for each country pair. We will show that by combining different measures of state preferences, and better accounting for network dependencies through our approach, we are able to generate a measure for preference that: maintains the insights of both UN voting scores and S-scores; and which can yield new insights, in particular, when it comes to predicting and explaining interstate conflict.

\subsection*{Latent Factor Model}

A number of techniques have been developed in recent years to estimate low-dimensional representations of network structures.\footnote{For a literature review of these approaches see \citet{goldenberg:etal:2010}.} For many of these approaches the goal is to account for higher order dependence pattern such as stochastic equivalence. Stochastic equivalence refers to the idea that there are communities of nodes in a network, and actors within a community act similarly towards those in other communities. Thus the community membership of an actor provides us with information on how that actor will act towards others in the network. Put more concretely, a pair of actors $ij$ are stochastically equivalent if the probability of $i$ relating to, and being related to, by every other actor is the same as the probability for $j$ \citep{anderson:etal:1992}. An additional dependence pattern that often manifests in networks is homophily -- the tendency of actors to form transitive links. The presence of homophily in a network implies that actors may cluster together because they share some latent attribute. In the context of clustering in alliance relationships, we are likely to find that states like the United States, United Kingdom, and Germany may cluster together because they share some latent state level attribute. Ignoring these patterns when generating a measure of state preferences is likely to paint an incomplete picture of the preferences that states share with one another.

We account for higher order dependence patterns using a latent factor model (LFM) that allows us to capture both concepts discussed above: the tendency of actors to assort themselves into groups and to form transitive links \citep{hoff:2007,minhas:etal:2016:arxiv}. Using the LFM ensures that similarity in preferences are likely to be transitive, for example, if the US has similar preferences to the UK, and the UK to France, the US's preferences should be relatively close to France's. The most useful feature of the LFM for our purpose is that it provides a visual interpretation of those interdependencies by inferring actor positions in a k-dimensional latent vector space. Actors that have vectors pointing in similar directions are more likely to have similar state preferences based on their alliances and UN voting records. The angle between the vectors for actors $i$ and $j$ provides an estimate of how similar the state preferences of $i$ are to $j$. 

To generate this measure we begin by constructing $T$ $n_{t} \times n_{t} \times p$ arrays ($Y_{t}$), where $T$ represents the number of periods, $n_{t}$ represents the number of actors in period $t$, and $p$ the number of observed variables used to synthesize a measure of state preference. Obtaining a lower-dimension relational measure of state preferences that captures network dependence patterns, we use the LFM to decompose each array: 

\begin{align*}
	Y_{t} &= f(\theta_{t})\\
	\theta_{t} &= \beta^{\top} X_{t} + Z_{t}
\end{align*}

where $f(.)$ is a general link function corresponding to the distribution of $Y_{t}$ and $\beta^{\top}\mathbf{X_{t}}$ is the standard regression term for dyadic and nodal fixed effects, and $Z_{t}$ represents any additional patterns in data unrelated to the specified dyadic and nodal fixed effects. In this application, for the sake of parsimony we abstain from using fixed effects. However, if one was interested in estimating a measure of preference that factored out the effect of distance, for example, than this could be accomplished within the context of this framework. 

The key part of this model lies in the decomposition of $Z_{t}$. \citet{hoff:2009} notes that we can write $Z_{t} = M + E$ such that the matrix $\mathbf{E}$ represents noise, and $\mathbf{M}$ is systematic effects. By matrix theory, we can factorize $M$ into the product of two simpler matrices: $\mathbf{M} = \mathbf{UDU^{\top}}$. $\mathbf{U}$ is a $n_{t} \times k$ and $\mathbf{D}$ is a $k \times k$ diagonal matrix -- $k$ is determined by the researcher. This is called the eigenvalue decomposition of $\mathbf{M}$. A Bayesian based procedure to determining the eigenvalue decomposition is available in the \pkg{amen} \textt{R} package \citep{amenpkg}. 

For the latent vector model, sample each ui from its (multivariate normal) full con- ditional distribution, sample the mean of the ui’s from their (normal) full conditional distributions, and then sample Λ from its (multivariate normal) full conditional distri- bution.

An inter- pretation of the latent eigenmodel is that each node i has a vector of unobserved characteristics ui = {ui,1 , . . . , ui,K }, and that similar values of ui,k and uj,k will contribute positively or nega- tively to the relationship between i and j, depending on whether λk > 0 or λk < 0. In this way, the model can represent both positive or negative homophily in varying degrees, and stochastically equivalent nodes (nodes with the same or similar latent vectors) may or may not have strong rela- tionships with one another.



% \begin{equation}
% 	P(Y_{i,j} = 1| x_{i,j}) = \beta^{'}\mathbf{x_{i,j}} + \mathbf{u_{i}Dv^{'}_{j}} + \epsilon_{i,j}
% \end{equation}

% The appeal of the singular value decomposition is partly due to its interpretation as a multi- plicative model based on row and column factors. Given a model of the form Y = M + E, the elements of Y can be written yi,j = u′iDvj +ei,j, where ui and vj are the ith and jth rows of U and V respectively. This model thus provides a representation of the systematic variation among the entries of Y by row and column factors. Models of this type play a role in the analysis of relational data (Harshman et al., 1982), biplots (Gabriel 1971, Gower and Hand 1996) and in reduced-rank interaction models for factorial designs (Gabriel 1978, 1998).

% \begin{equation}
% 	P(Y_{i,j} = 1| x_{i,j}) = \beta^{'}\mathbf{x_{i,j}} + \mathbf{u_{i}Dv^{'}_{j}} + \epsilon_{i,j}
% \end{equation}

% An important innovation with the AME, as compared to previous network estimates is the ability to handle replicated datasets -- here we use the replicated dataset to incorporate multiple measures of similarity into a single ideal point estimation.  The AME with dyadic data treats each different slice of data as independent, save for those dependencies captured by the multiplicative random effects. The final estimating equation we use is:

% \begin{equation}
% 	P(Y_{i,j_j} = 1) = \mu + \mathbf{u_{i}Dv^{'}_{j}} + \epsilon_{i,j,t}
% \end{equation}

% [Describe how to intepret SVD stuff based off cosine similarity]

% What is particularly useful here is the eponymous multiplicative effect $\mathbf{u_{i}Dv^{'}_{j}}$. This effect not only helps to account for homophily and stochastic equivalence, it also places each state in a latent space. What is key to understand about this latent space is that it is non-euclidian. Rather than have states behave similar to the states which are close to them, this latent space is a two dimensional representation of a hypersphere, and thus states are apt to behave similarly to the states that are placed in the same direction on said sphere. Thus, if two states vectors (from the center of the space) are in the same direction, they are apt to send and receive both alliances and co-voting to similar targets. The way we measure this similarity in dimension is by looking at the absolute distance of the angles created by each states position and the center of the latent space. 

% https://books.google.com/books?id=Bb0UBQAAQBAJ&pg=PA101&lpg=PA101&dq=latent+factor,+cosine+angle&source=bl&ots=rGdBVz7jJV&sig=pD8XM9e_nVJhga89Bnn7jw7AQ5w&hl=en&sa=X&ved=0ahUKEwics8ruqN3UAhUFwmMKHUf8Av8Q6AEINTAD#v=onepage&q=latent%20factor%2C%20cosine%20angle&f=false
% To define diversity let us revisit the latent factor representation of users and products obtained through matrix factorization: each user and product is represented by a vector of latent factors, where these latent factors are abstract or real=life characteristics. This means that a product vector's direction within the latent factor high-dimensional space indicates which characteristics this product has and which it does not have. Hence two products can be compared by comparing the direction to which their respective factor vectors point. In this sense, the straightforward choice for a comparison metric between products is the cosine of the angle formed by these products. This metric is well known as the cosine similarity/distance, defined as $cosSim_{a,b} = \frac{p_{a} \cdot p_{b}}{||p_{a}|| \cdot ||p_{b}||}$. Here $p_{a}$ and $p_{b}$ denote products $a$ and $b$ factor vectors. Notice that cosine similarity ranges from -1 to 1. Using the cosine similarity metric our intuition is that a set of 2 products will be more diversified if the cosine similarity between its product is negative or close to zero. Figure 1 illustrates the intuition behind cosine diversity. Accounting for vertex directions by measuring the diversity in terms of the sums of the vertex angles avoids the curse of high-dimensionality problem and is more consistent with what we consider to be diversity. 


% https://cambridgespark.com/content/tutorials/implementing-your-own-recommender-systems-in-Python/index.html

% A distance metric commonly used in recommender systems is cosine similarity, where the ratings are seen as vectors in nn-dimensional space and the similarity is calculated based on the angle between these vectors. Cosine similiarity for users aa and mm can be calculated using the formula below, where you take dot product of the user vector ukuk and the user vector uaua and divide it by multiplication of the Euclidean lengths of the vectors.

% scosu(uk,ua)=uk⋅ua‖uk‖‖ua‖=∑xk,mxa,m∑x2k,m∑x2a,m‾‾‾‾‾‾‾‾‾‾‾‾‾√
% sucos(uk,ua)=uk⋅ua‖uk‖‖ua‖=∑xk,mxa,m∑xk,m2∑xa,m2

% To calculate similarity between items mm and bb you use the formula:

% scosu(im,ib)=im⋅ib‖im‖‖ib‖=∑xa,mxa,b∑x2a,m∑x2a,b‾‾‾‾‾‾‾‾‾‾‾‾√