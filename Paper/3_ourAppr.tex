\subsection*{Synthesizing Measures of State Preference}

The approach that we introduce here to measuring state preferences begins by recognizing that both UN voting and alliance relationships are both representation of how states relate to one another in the international stage. And by accounting for the multiple layers upon which states interact with one another we can then synthesize a better measure of state preferences.  The idea of using multiple metrics to get a better handle on preferences is not new, in fact, Signorino and Ritter suggested it in introducing S scores, which were designed to allow for aggregation of similarity on multiple dimensions (such as alliances and UN voting). What we propose, is to combine the dyadic measures of state similarity created using S-scores for alliances, and using voting models for UN data, in a manner that is both principled, and allows us to account for network level patterns that can be used to more fully explain state preferences. 

In particular, we use these two measures of state preference in a latent factor model, in order to ascertain the state positions that best explain not only states dyadic similarity and dissimilarity on the two measures, but also why states form the clusters they form. Our hope, is that by combining different measures of state preferences, and better accounting for spatial dependencies, we are able to generate a measure for preference that maintains the insights of both UN voting scores and S-scores, but which can also yield some new insights, in particular, when it comes to predicting and explaining interstate conflict.

% needs to be modified
\begin{figure}[ht]
	\centering
	\includegraphics[width=.8\textwidth]{carbonStrat.png}
	\caption{Tensor representation of longitudinal dyadic, representational measures. The green and blue colors represent different relational measures and darker shading indicates later time periods. Specifically, we show a tensor with dimensions of $4 \times 4 \times 2 \times 3$, where 4 represents the number of actors, 2 the number of relational measures, and 3 the number of time points.}
	\label{fig:tensViz}
\end{figure}

\section*{Methodology}

% \subsection*{Latent Factor Model}

% [Many options available to begin working with multirelational networks. a notable corpus of literature has been developed using community detection approaches that have their origins in the latent class models developed by \citet{nowicki:snijders:2001}. ]

% [We choose to utilize a model based singular value decompositioon approach. This approach allows us to capture two concepts that often emerge in networks: the tendency of actors to form transitive links and the tendency of actors to assort themselves into broader groups (ie, homophily and stochastic equivalence). \citet{hoff:2007} shows that such an approach generalizes  the latent class model. ]

% Let Z be an n × n random matrix of effects representing deviations of the log-odds θi,j from the linear predictor β′xi,j. We can write Z = M+E, where the mean matrix M represents systematic patterns in the effects and E represents noise. A basic result from matrix theory is that every n × n matrix M has the representation

% The appeal of the singular value decomposition is partly due to its interpretation as a multi- plicative model based on row and column factors. Given a model of the form Y = M + E, the elements of Y can be written yi,j = u′iDvj +ei,j, where ui and vj are the ith and jth rows of U and V respectively. This model thus provides a representation of the systematic variation among the entries of Y by row and column factors. Models of this type play a role in the analysis of relational data (Harshman et al., 1982), biplots (Gabriel 1971, Gower and Hand 1996) and in reduced-rank interaction models for factorial designs (Gabriel 1978, 1998).

% We want a model that takes a set of actions between countries, and infers each countries position in a latent preference space, such that those countries close to each other are likely to have similar preferences and therefore have similar alliances and UN voting records. We would like this methodology to be able to, in a principled way combine different sources of data, for example imputing ideal points based on both alliance behavior and behavior at the UN. Finally, and importantly, this method should be able to account for interdependencies: similarity in preferences should be transitive (if the US has similar preferences to the UK, and the UK to France, the US's preferences should be relatively close to France's) and should allow for clusters of states with similar preferences.

% The latent factor model is a relatively new technique that is a recent generalization of the Generalized Bilinear Mixed-Effects model from \citet{hoff:2005}. 

% \begin{equation}
% 	f(Y_{i,j}) =  \beta^{'}\mathbf{x_{i,j}} + \alpha_{i} + b_{j} + \epsilon_{i,j}
% \end{equation}

% where $f(.)$ is a general link function corresponding to the distribution of Y, $\beta^{'}\mathbf{x_{i,j}}$ is the standard regression term for dyadic and nodal fixed effects,  $\alpha_{i}, b_{j}$ are sender and receiver random effects, and $\epsilon_{i,j}$ is an IID error term. The AME model further decomposes the  error term as follows. If we assume the matrix representation of deviation from the linear predictors and random effects is $\mathbf{Z}$, then $\mathbf{Z} = \mathbf{M} + \mathbf{E}$ such that the matrix $\mathbf{E}$ represents noise, and $\mathbf{M}$ is systematic effects. By matrix theory, we can decompose $\mathbf{M} = \mathbf{UDV^{'}}$ such that $\mathbf{U}$ and $\mathbf{V}$ are are n x n matrices with othonormal columns, and $\mathbf{D}$ is an n x n diagonal matrix. This is called the singular value decomposition of $\mathbf{M}$. 

% We then write the AME model for a given value $Y_{i,j} \in \{0,1\}$:\footnote{An alternative formulation of this model includes nodal sender and receiver random effects along with an error covariance structure based on the social relations model: $P(Y_{i,j} = 1| x_{i,j}) = \beta^{'}\mathbf{x_{i,j}} + \alpha_{i} + b_{j} + \mathbf{u_{i}Dv^{'}_{j}} + \epsilon_{i,j}$. }

% \begin{equation}
% 	P(Y_{i,j} = 1| x_{i,j}) = \beta^{'}\mathbf{x_{i,j}} + \mathbf{u_{i}Dv^{'}_{j}} + \epsilon_{i,j}
% \end{equation}

% In estimating preference models, we abstain from using fixed effects save an intercept.

% An important innovation with the AME, as compared to previous network estimates is the ability to handle replicated datasets -- here we use the replicated dataset to incorporate multiple measures of similarity into a single ideal point estimation.  The AME with dyadic data treats each different slice of data as independent, save for those dependencies captured by the nodal and multiplicative random effects, as well as those controlled for by fixed effects. The final estimating equation we use is:

% \begin{equation}
% 	P(Y_{i,j_j} = 1) = \mu + \mathbf{u_{i}Dv^{'}_{j}} + \epsilon_{i,j,t}
% \end{equation}

% [Describe how to intepret SVD stuff based off cosine similarity]

% What is particularly useful here is the eponymous multiplicative effect $\mathbf{u_{i}Dv^{'}_{j}}$. This effect not only helps to account for homophily and stochastic equivalence, it also places each state in a latent space. What is key to understand about this latent space is that it is non-euclidian. Rather than have states behave similar to the states which are close to them, this latent space is a two dimensional representation of a hypersphere, and thus states are apt to behave similarly to the states that are placed in the same direction on said sphere. Thus, if two states vectors (from the center of the space) are in the same direction, they are apt to send and receive both alliances and co-voting to similar targets. The way we measure this similarity in dimension is by looking at the absolute distance of the angles created by each states position and the center of the latent space. 

% https://books.google.com/books?id=Bb0UBQAAQBAJ&pg=PA101&lpg=PA101&dq=latent+factor,+cosine+angle&source=bl&ots=rGdBVz7jJV&sig=pD8XM9e_nVJhga89Bnn7jw7AQ5w&hl=en&sa=X&ved=0ahUKEwics8ruqN3UAhUFwmMKHUf8Av8Q6AEINTAD#v=onepage&q=latent%20factor%2C%20cosine%20angle&f=false
% To define diversity let us revisit the latent factor representation of users and products obtained through matrix factorization: each user and product is represented by a vector of latent factors, where these latent factors are abstract or real=life characteristics. This means that a product vector's direction within the latent factor high-dimensional space indicates which characteristics this product has and which it does not have. Hence two products can be compared by comparing the direction to which their respective factor vectors point. In this sense, the straightforward choice for a comparison metric between products is the cosine of the angle formed by these products. This metric is well known as the cosine similarity/distance, defined as $cosSim_{a,b} = \frac{p_{a} \cdot p_{b}}{||p_{a}|| \cdot ||p_{b}||}$. Here $p_{a}$ and $p_{b}$ denote products $a$ and $b$ factor vectors. Notice that cosine similarity ranges from -1 to 1. Using the cosine similarity metric our intuition is that a set of 2 products will be more diversified if the cosine similarity between its product is negative or close to zero. Figure 1 illustrates the intuition behind cosine diversity. Accounting for vertex directions by measuring the diversity in terms of the sums of the vertex angles avoids the curse of high-dimensionality problem and is more consistent with what we consider to be diversity. 


% https://cambridgespark.com/content/tutorials/implementing-your-own-recommender-systems-in-Python/index.html

% A distance metric commonly used in recommender systems is cosine similarity, where the ratings are seen as vectors in nn-dimensional space and the similarity is calculated based on the angle between these vectors. Cosine similiarity for users aa and mm can be calculated using the formula below, where you take dot product of the user vector ukuk and the user vector uaua and divide it by multiplication of the Euclidean lengths of the vectors.

% scosu(uk,ua)=uk⋅ua‖uk‖‖ua‖=∑xk,mxa,m∑x2k,m∑x2a,m‾‾‾‾‾‾‾‾‾‾‾‾‾√
% sucos(uk,ua)=uk⋅ua‖uk‖‖ua‖=∑xk,mxa,m∑xk,m2∑xa,m2

% To calculate similarity between items mm and bb you use the formula:

% scosu(im,ib)=im⋅ib‖im‖‖ib‖=∑xa,mxa,b∑x2a,m∑x2a,b‾‾‾‾‾‾‾‾‾‾‾‾√